---
title: 'Class 4: Power and Effect size'
geometry: margin=.5in
output:
  pdf_document:
    highlight: default
    number_sections: yes
    toc: yes
    toc_depth: 3
  html_document:
    toc: yes
    toc_depth: '3'
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning =  FALSE)
knitr::opts_chunk$set(fig.width=4.25)
knitr::opts_chunk$set(fig.height=4.0)
knitr::opts_chunk$set(fig.align='center') 
knitr::opts_chunk$set(fig.pos = 'H')
knitr::opts_chunk$set(results='hold') 
```


\pagebreak

Note: There is some very complex R code used to generate today's lecture. I have hidden it in the PDF file. If you see `echo=FALSE` inside the RMD file it means that is the code you are not expected to understand or learn. I will explain the functions you will need to learn. 

# Probability is a Fickle 

Review: The standard error can be understood as the Standard deviation of Sample Means. To understand this let's generate a population for the Advanced Measures of Music Audiation (a test of college students tonal and rhythmic discrimination), $\mu$ = 27, $\sigma$ = 3.75 and view the PDF of the population. We can see below 95% of the X scores occur between about 19.65 & 34.35 on rhythmic discrimination. 

```{r, echo=FALSE}
# modified from https://www.statmethods.net/advgraphs/probability.html
mean=27; sd=3.75
lb=(mean - sd*1.96); ub=(mean + sd*1.96)

x <- seq(-4,4,length=100)*sd + mean
hx <- dnorm(x,mean,sd)

plot(x, hx, type="n", xlab="Rhythmic Score", ylab="",
  main="", axes=FALSE)

i <- x >= lb & x <= ub
lines(x, hx)
polygon(c(lb,x[i],ub), c(0,hx[i],0), col="red") 

area <- pnorm(ub, mean, sd) - pnorm(lb, mean, sd)
result <- paste("P(",lb,"< Rhythm <",ub,") =",
   signif(area, digits=3))
mtext(result,3)
axis(1, at=seq(0, 40, 2), pos=0)
```

Now lets draw 10 samples of $n = 13$ people per sample. In other words, 10 data collections with 13 people where we measure their rhythmic discrimination 

```{r, echo=FALSE}
set.seed(543)
N=13; M=27; SD=3.75; samples=10
Sample.Means<-replicate(samples, mean(rnorm(n=N,mean=M,sd=SD)))
hist(Sample.Means,
  main="Histogram", 
  xlab="Rhythmic discrimination of 10 samples (n = 12)", ylab="Frequency",
  xlim=c(20,30))
```



The mean of the means of the samples was `r round(mean(Sample.Means),3)`, close to the mean of the popluation, $\mu=27$. The predicted standard error based on our equation $\sigma_M = \frac{\sigma}{\sqrt{n}} = \frac{3.75}{\sqrt{13}}$ = `r round(3.75/(13^.5),2)`.  In reality these 10 studies yielded SD(sample means) = `r round(sd(Sample.Means),2)`. **Why do different?** 

The problem is our original sample was too small and we probably got only people in the middle by chance. So what if we run 5000 replications of what we just did:  SD(10 replications of 13 people per sample). The red line = theoretical SEM


```{r, echo=FALSE}
set.seed(543)
N=13; M=27; SD=3.75; samples=10
Sample.SD<-replicate(5000, sd(replicate(samples, mean(rnorm(n=N,mean=M,sd=SD)))))
hist(Sample.SD,
  main="Histogram", 
  xlab="Empirical SEM: \n5000 replications SD(10 studies of n = 13)", ylab="Frequency",
  xlim=c(0,2))
abline(v=(3.75/(13^.5)),col="red",lwd=4)

```

You will notice the graph is not symmetrical.  In fact, `r round(mean(ifelse(Sample.SD < (3.75/(13^.5)),1,0)),2)*100`% of the scores are below the theoretical SEM and `r round(mean(ifelse(Sample.SD > (3.75/(13^.5)),1,0)),2)*100`% are above the theoretical SEM. Why did this happen? For the same reason, we got a low estimate before: *with the small sample we are most likely getting people only from the middle of the population because they are most likely*.  However, sometimes by chance, you will get scores (and means) that are WAY too small or large.  This the problem with Gosset introduced by moving us away from using known values of $\sigma$ standard deviation and using the sample $S$ as an approximation. The solution to this problem is to estimate how likely you are to get your result by chance given your *specific sample size* and given how *big your effect* is likely to be.

## Signal Detection Theory
Given I must estimate my SEM from my $S$, I need a way to see how "big" an effect might be. Cohen borrowed a useful idea at the time, signal detection theory which was created during the invention of the radio and radar.  The idea was that you want to know how strong a signal needed to be for you to detect it over the noise. The noise for us is the control group and the signal the effect of your treatment.  

```{r, echo=FALSE, fig.width=6.25,fig.height=3.0}
# modifed from http://rpsychologist.com/creating-a-typical-textbook-illustration-of-statistical-power-using-either-ggplot-or-base-graphics

library(ggplot2)
library(grid) # need for arrow()
m1 <- 0  # mu H0
sd1 <- 1.5 # sigma H0
m2 <- 3.5 # mu HA
sd2 <- 1.5 # sigma HA

z_crit <- qnorm(1-(0.05/2), m1, sd1)

# set length of tails
min1 <- m1-sd1*4
max1 <- m1+sd1*4
min2 <- m2-sd2*4
max2 <- m2+sd2*4          
# create x sequence
x <- seq(min(min1,min2), max(max1, max2), .01)
# generate normal dist #1
y1 <- dnorm(x, m1, sd1)
# put in data frame
df1 <- data.frame("x" = x, "y" = y1)
# generate normal dist #2
y2 <- dnorm(x, m2, sd2)
# put in data frame
df2 <- data.frame("x" = x, "y" = y2)

# combine polygons. 
df1$id <- 2 
df2$id <- 1 
poly <- rbind(df1, df2)
poly$id <- factor(poly$id,  labels=c("h0","h1"))

ggplot(poly, aes(x,y,  group=id)) +
  geom_polygon(show_guide=F, alpha=I(8/10)) +
  geom_line(data=df1, aes(x,y, color="H0", group=NULL, fill=NULL), size=1.5, show_guide=F) + 
  geom_line(data=df2, aes(color="HA", group=NULL, fill=NULL),size=1.5, show_guide=F) +
  scale_color_manual("Group", 
                     values= c("HA" = "#981e0b","H0" = "black")) +
  annotate("segment", x=m1+.4, y=0.265, xend=m2-.4, yend=0.265, color='blue',
           arrow = arrow(length = unit(0.3, "cm")), size=1) +
  annotate("text", label="d", x=(m1+m2)/2, y=0.25, parse=T, size=8) +
  annotate("segment", x=m2-.4, y=0.265, xend=m1+.4, yend=0.265, color='blue',
           arrow = arrow(length = unit(0.3, "cm")), size=1) +
  annotate("text", label="N", x=m1, y=0.28, parse=T, size=8) +
  annotate("text", label="N + S", x=m2, y=0.28, parse=T, size=8) +
  ggtitle("Signal Detection") +
  theme(panel.grid.minor = element_blank(),
             panel.grid.major = element_blank(),
             panel.background = element_blank(),
             plot.background = element_rect(fill="#ffffff"),
             panel.border = element_blank(),
             axis.line = element_blank(),
             axis.text.x = element_blank(),
             axis.text.y = element_blank(),
             axis.ticks = element_blank(),
             axis.title.x = element_blank(),
             axis.title.y = element_blank(),
             plot.title = element_text(size=22))
```

$$ d = \frac{\mu_{(Noise+Signal)}-\mu_{(Noise)}}{\sigma_{(Noise)}} $$
Note this implicit assumption: $$\sigma_{(Noise)} = \sigma_{(Noise+Signal})$$

## Cohen's $d$
Cohen surmised that we could apply the same logic to experiments to determine how big the effect would be (in other words how easy it is to distinguish the signal from the noise). 

$$ d = \frac{M_{H1}-\mu_{H0}}{S_{H1}} $$

You might notice you have seen this formula before!

$$ Z = \frac{M-\mu}{S} $$

So Cohen's $d$ is in standard deviation units:

Size   | $d$
------ | --------
Small  | .2
Medium | .5
Large  | .8
              
### Small Visualized
```{r, echo=FALSE, fig.width=4.25,fig.height=2.0}
# modified from http://rpsychologist.com/creating-a-typical-textbook-illustration-of-statistical-power-using-either-ggplot-or-base-graphics

library(ggplot2)
library(grid) # need for arrow()
m1 <- 0  # mu H0
sd1 <- 1 # sigma H0
m2 <- .2 # mu HA
sd2 <- 1 # sigma HA

z_crit <- qnorm(1-(0.05/2), m1, sd1)

# set length of tails
min1 <- m1-sd1*4
max1 <- m1+sd1*4
min2 <- m2-sd2*4
max2 <- m2+sd2*4          
# create x sequence
x <- seq(min(min1,min2), max(max1, max2), .01)
# generate normal dist #1
y1 <- dnorm(x, m1, sd1)
# put in data frame
df1 <- data.frame("x" = x, "y" = y1)
# generate normal dist #2
y2 <- dnorm(x, m2, sd2)
# put in data frame
df2 <- data.frame("x" = x, "y" = y2)

# combine polygons. 
df1$id <- 2 
df2$id <- 1 
poly <- rbind(df1, df2)
poly$id <- factor(poly$id,  labels=c("h0","h1"))

ggplot(poly, aes(x,y,  group=id)) +
  geom_polygon(show_guide=F, alpha=I(8/10)) +
  geom_line(data=df1, aes(x,y, color="H0", group=NULL, fill=NULL), size=1.5, show_guide=F) + 
  geom_line(data=df2, aes(color="HA", group=NULL, fill=NULL),size=1.5, show_guide=F) +
  scale_color_manual("Group", 
                     values= c("HA" = "#981e0b","H0" = "black")) +
  ggtitle("Small Effect") +
  theme(panel.grid.minor = element_blank(),
             panel.grid.major = element_blank(),
             panel.background = element_blank(),
             plot.background = element_rect(fill="#ffffff"),
             panel.border = element_blank(),
             axis.line = element_blank(),
             axis.text.x = element_blank(),
             axis.text.y = element_blank(),
             axis.ticks = element_blank(),
             axis.title.x = element_blank(),
             axis.title.y = element_blank(),
             plot.title = element_text(size=22))
```

### Medium Visualized
```{r, echo=FALSE, fig.width=4.25,fig.height=2.0}
# modified from http://rpsychologist.com/creating-a-typical-textbook-illustration-of-statistical-power-using-either-ggplot-or-base-graphics

library(ggplot2)
library(grid) # need for arrow()
m1 <- 0  # mu H0
sd1 <- 1 # sigma H0
m2 <- .5 # mu HA
sd2 <- 1 # sigma HA

z_crit <- qnorm(1-(0.05/2), m1, sd1)

# set length of tails
min1 <- m1-sd1*4
max1 <- m1+sd1*4
min2 <- m2-sd2*4
max2 <- m2+sd2*4          
# create x sequence
x <- seq(min(min1,min2), max(max1, max2), .01)
# generate normal dist #1
y1 <- dnorm(x, m1, sd1)
# put in data frame
df1 <- data.frame("x" = x, "y" = y1)
# generate normal dist #2
y2 <- dnorm(x, m2, sd2)
# put in data frame
df2 <- data.frame("x" = x, "y" = y2)

# combine polygons. 
df1$id <- 2 
df2$id <- 1 
poly <- rbind(df1, df2)
poly$id <- factor(poly$id,  labels=c("h0","h1"))

ggplot(poly, aes(x,y,  group=id)) +
  geom_polygon(show_guide=F, alpha=I(8/10)) +
  geom_line(data=df1, aes(x,y, color="H0", group=NULL, fill=NULL), size=1.5, show_guide=F) + 
  geom_line(data=df2, aes(color="HA", group=NULL, fill=NULL),size=1.5, show_guide=F) +
  scale_color_manual("Group", 
                     values= c("HA" = "#981e0b","H0" = "black")) +
  ggtitle("Medium Effect") +
  theme(panel.grid.minor = element_blank(),
             panel.grid.major = element_blank(),
             panel.background = element_blank(),
             plot.background = element_rect(fill="#ffffff"),
             panel.border = element_blank(),
             axis.line = element_blank(),
             axis.text.x = element_blank(),
             axis.text.y = element_blank(),
             axis.ticks = element_blank(),
             axis.title.x = element_blank(),
             axis.title.y = element_blank(),
             plot.title = element_text(size=22))
```

### Large Visualized
```{r, echo=FALSE, fig.width=4.25,fig.height=2.0}
# modifed from http://rpsychologist.com/creating-a-typical-textbook-illustration-of-statistical-power-using-either-ggplot-or-base-graphics

library(ggplot2)
library(grid) # need for arrow()
m1 <- 0  # mu H0
sd1 <- 1 # sigma H0
m2 <- .8 # mu HA
sd2 <- 1 # sigma HA

z_crit <- qnorm(1-(0.05/2), m1, sd1)

# set length of tails
min1 <- m1-sd1*4
max1 <- m1+sd1*4
min2 <- m2-sd2*4
max2 <- m2+sd2*4          
# create x sequence
x <- seq(min(min1,min2), max(max1, max2), .01)
# generate normal dist #1
y1 <- dnorm(x, m1, sd1)
# put in data frame
df1 <- data.frame("x" = x, "y" = y1)
# generate normal dist #2
y2 <- dnorm(x, m2, sd2)
# put in data frame
df2 <- data.frame("x" = x, "y" = y2)

# combine polygons. 
df1$id <- 2 
df2$id <- 1 
poly <- rbind(df1, df2)
poly$id <- factor(poly$id,  labels=c("h0","h1"))

ggplot(poly, aes(x,y,  group=id)) +
  geom_polygon(show_guide=F, alpha=I(8/10)) +
  geom_line(data=df1, aes(x,y, color="H0", group=NULL, fill=NULL), size=1.5, show_guide=F) + 
  geom_line(data=df2, aes(color="HA", group=NULL, fill=NULL),size=1.5, show_guide=F) +
  scale_color_manual("Group", 
                     values= c("HA" = "#981e0b","H0" = "black")) +
  ggtitle("Large Effect") +
  theme(panel.grid.minor = element_blank(),
             panel.grid.major = element_blank(),
             panel.background = element_blank(),
             plot.background = element_rect(fill="#ffffff"),
             panel.border = element_blank(),
             axis.line = element_blank(),
             axis.text.x = element_blank(),
             axis.text.y = element_blank(),
             axis.ticks = element_blank(),
             axis.title.x = element_blank(),
             axis.title.y = element_blank(),
             plot.title = element_text(size=22))
```

You notice that even in a large effect there is a lot of overlap between the curves. 

# Power
Power is the likelihood that a study will detect an effect when there is an effect. Power = $1 - \beta$, where $\beta$ = Type II error (Cohen, 1962, 1988, 1992). Below is a visual representation of power. 

```{r, echo=FALSE, fig.width=6.25,fig.height=3.0}
m1 <- 0  # mu H0
sd1 <- 1.5 # sigma H0
m2 <- 3.5 # mu HA
sd2 <- 1.5 # sigma HA

z_crit <- qnorm(1-(0.05/2), m1, sd1)

# set length of tails
min1 <- m1-sd1*4
max1 <- m1+sd1*4
min2 <- m2-sd2*4
max2 <- m2+sd2*4          
# create x sequence
x <- seq(min(min1,min2), max(max1, max2), .01)
# generate normal dist #1
y1 <- dnorm(x, m1, sd1)
# put in data frame
df1 <- data.frame("x" = x, "y" = y1)
# generate normal dist #2
y2 <- dnorm(x, m2, sd2)
# put in data frame
df2 <- data.frame("x" = x, "y" = y2)

# Alpha polygon
y.poly <- pmin(y1,y2)
poly1 <- data.frame(x=x, y=y.poly)
poly1 <- poly1[poly1$x >= z_crit, ] 
poly1<-rbind(poly1, c(z_crit, 0))  # add lower-left corner

# Beta polygon
poly2 <- df2
poly2 <- poly2[poly2$x <= z_crit,] 
poly2<-rbind(poly2, c(z_crit, 0))  # add lower-left corner

# power polygon; 1-beta
poly3 <- df2
poly3 <- poly3[poly3$x >= z_crit,] 
poly3 <-rbind(poly3, c(z_crit, 0))  # add lower-left corner

# combine polygons. 
poly1$id <- 3 # alpha, give it the highest number to make it the top layer
poly2$id <- 2 # beta
poly3$id <- 1 # power; 1 - beta
poly <- rbind(poly1, poly2, poly3)
poly$id <- factor(poly$id,  labels=c("power","beta","alpha"))

ggplot(poly, aes(x,y, fill=id, group=id)) +
  geom_polygon(show_guide=F, alpha=I(8/10)) +
  # add line for treatment group
  geom_line(data=df1, aes(x,y, color="H0", group=NULL, fill=NULL), size=1.5, show_guide=F) + 
  # add line for treatment group. These lines could be combined into one dataframe.
  geom_line(data=df2, aes(color="HA", group=NULL, fill=NULL),size=1.5, show_guide=F) +
  # add vlines for z_crit
  geom_vline(xintercept = z_crit, size=1, linetype="dashed") +
  # change colors 
  scale_color_manual("Group", 
                     values= c("HA" = "#981e0b","H0" = "black")) +
  scale_fill_manual("test", values= c("alpha" = "#0d6374","beta" = "#be805e","power"="#7cecee")) +
  # beta arrow
  annotate("segment", x=0.1, y=0.045, xend=1.3, yend=0.01, arrow = arrow(length = unit(0.3, "cm")), size=1) +
  annotate("text", label="beta", x=0, y=0.05, parse=T, size=8) +
  # alpha arrow
  annotate("segment", x=4, y=0.043, xend=3.4, yend=0.01, arrow = arrow(length = unit(0.3, "cm")), size=1) +
  annotate("text", label="frac(alpha,2)", x=4.2, y=0.05, parse=T, size=8) +
  # power arrow
  annotate("segment", x=6, y=0.2, xend=4.5, yend=0.15, arrow = arrow(length = unit(0.3, "cm")), size=1) +
  annotate("text", label="1-beta", x=6.1, y=0.21, parse=T, size=8) +
  # H_0 title
  annotate("text", label="H[0]", x=m1, y=0.28, parse=T, size=8) +
  # H_a title
  annotate("text", label="H[1]", x=m2, y=0.28, parse=T, size=8) +
  ggtitle("Power Example") +
  # remove some elements
  theme(panel.grid.minor = element_blank(),
             panel.grid.major = element_blank(),
             panel.background = element_blank(),
             plot.background = element_rect(fill="#ffffff"),
             panel.border = element_blank(),
             axis.line = element_blank(),
             axis.text.x = element_blank(),
             axis.text.y = element_blank(),
             axis.ticks = element_blank(),
             axis.title.x = element_blank(),
             axis.title.y = element_blank(),
             plot.title = element_text(size=22))
```


## Uses of Power
**A priori Power:** Setting a specific likelihood (power = .80) at given $\alpha$ level (.05), knowing the *true* effect size. This is used to estimate the sample size needed to achieve that power (Cohen, 1962, 1988, 1992). The problem here is what is the *true* effect size? We used to say go find a meta-analysis to estimate it, but those effect sizes are overestimated because of publication bias (Kicinski et al., 2015). Some other rules of thumb, assume your effect is small-to-medium (d = .3 or .4). 

**Post-hoc Power:** Estimating the power you achieved given the *observed* effect size you found from your study, the sample size and $\alpha$ you set.  Post-hoc power has no meaning, and the reason has to do probability being fickle (Hoenig & Heisey, 2001). In short, the problem is that your estimate of $\sigma$ in small samples, $S$ is going to be underestimated making your calculation of $d$ too large.  I will show you in graphs how non-trivial this issue is.  


## Power and Significance testing
While I have shown you power as it related to population $\mu$ & $\sigma$ distributions, we calculate significance testing based on 'standard error' units not 'standard deviation' units.  A reminder is that this means when we say something is statistically significant it does NOT mean it is clinically significant. Example, in "Simpson and Delilah" from the Simpsons (S7:E2), Homer takes Dimoxinil, and the next morning he has a full luscious head of hair. The effect of Dimoxinil is huge! In reality, Minoxidil (the real drug) grows back very little hair (after months and months of 2x a day applications). The effect is small, but it is significant because "most" men experience hair growth beyond noise which is defined in SEM not SD.  

As N increases, the standard error goes down ($\frac{S}{\sqrt{n}}$), meaning the width the noise decreases.

### Medium Effect Size
Curves = standard deviation

```{r, echo=FALSE, fig.width=4.25,fig.height=2.0}
# modified from http://rpsychologist.com/creating-a-typical-textbook-illustration-of-statistical-power-using-either-ggplot-or-base-graphics

library(ggplot2)
library(grid) # need for arrow()
m1 <- 0  # mu H0
sd1 <- 1 # sigma H0
m2 <- .5 # mu HA
sd2 <- 1 # sigma HA

z_crit <- qnorm(1-(0.05/2), m1, sd1)

# set length of tails
min1 <- m1-sd1*4
max1 <- m1+sd1*4
min2 <- m2-sd2*4
max2 <- m2+sd2*4          
# create x sequence
x <- seq(min(min1,min2), max(max1, max2), .01)
# generate normal dist #1
y1 <- dnorm(x, m1, sd1)
# put in data frame
df1 <- data.frame("x" = x, "y" = y1)
# generate normal dist #2
y2 <- dnorm(x, m2, sd2)
# put in data frame
df2 <- data.frame("x" = x, "y" = y2)

# combine polygons. 
df1$id <- 2 
df2$id <- 1 
poly <- rbind(df1, df2)
poly$id <- factor(poly$id,  labels=c("h0","h1"))

ggplot(poly, aes(x,y,  group=id)) +
  geom_polygon(show_guide=F, alpha=I(8/10)) +
  geom_line(data=df1, aes(x,y, color="H0", group=NULL, fill=NULL), size=1.5, show_guide=F) + 
  geom_line(data=df2, aes(color="HA", group=NULL, fill=NULL),size=1.5, show_guide=F) +
  scale_color_manual("Group", 
                     values= c("HA" = "#981e0b","H0" = "black")) +
  ggtitle("Medium Effect") +
  theme(panel.grid.minor = element_blank(),
             panel.grid.major = element_blank(),
             panel.background = element_blank(),
             plot.background = element_rect(fill="#ffffff"),
             panel.border = element_blank(),
             axis.line = element_blank(),
             axis.text.x = element_blank(),
             axis.text.y = element_blank(),
             axis.ticks = element_blank(),
             axis.title.x = element_blank(),
             axis.title.y = element_blank(),
             plot.title = element_text(size=22))
```

### Significance Test on Medium Effect size
The Distance of the peaks of the curves will not change (effect size based on the graph above), but we will change the width of the curve to reflect Standard Error 

#### Significance Test when N = 12
```{r, echo=FALSE, fig.width=4.25,fig.height=2.0}
# modified from http://rpsychologist.com/creating-a-typical-textbook-illustration-of-statistical-power-using-either-ggplot-or-base-graphics

library(ggplot2)
library(grid) # need for arrow()
m1 <- 0  # mu H0
sd1 <- 1 # sigma H0
m2 <- .5 # mu HA
sd2 <- 1 # sigma HA

z_crit <- qnorm(1-(0.05/2), m1, sd1)

# set length of tails
min1 <- m1-sd1*4
max1 <- m1+sd1*4
min2 <- m2-sd2*4
max2 <- m2+sd2*4          
# create x sequence
x <- seq(min(min1,min2), max(max1, max2), .01)
# generate normal dist #1
y1 <- dnorm(x, m1, sd1/6^.5)
# put in data frame
df1 <- data.frame("x" = x, "y" = y1)
# generate normal dist #2
y2 <- dnorm(x, m2, sd2/6^.5)
# put in data frame
df2 <- data.frame("x" = x, "y" = y2)

# combine polygons. 
df1$id <- 2 
df2$id <- 1 
poly <- rbind(df1, df2)
poly$id <- factor(poly$id,  labels=c("h0","h1"))

ggplot(poly, aes(x,y,  group=id)) +
  geom_polygon(show_guide=F, alpha=I(8/10)) +
  geom_line(data=df1, aes(x,y, color="H0", group=NULL, fill=NULL), size=1.5, show_guide=F) + 
  geom_line(data=df2, aes(color="HA", group=NULL, fill=NULL),size=1.5, show_guide=F) +
  scale_color_manual("Group", 
                     values= c("HA" = "#981e0b","H0" = "black")) +
  ggtitle("N = 12") +
  theme(panel.grid.minor = element_blank(),
             panel.grid.major = element_blank(),
             panel.background = element_blank(),
             plot.background = element_rect(fill="#ffffff"),
             panel.border = element_blank(),
             axis.line = element_blank(),
             axis.text.x = element_blank(),
             axis.text.y = element_blank(),
             axis.ticks = element_blank(),
             axis.title.x = element_blank(),
             axis.title.y = element_blank(),
             plot.title = element_text(size=22))
```

#### Significance Test when N = 24
```{r, echo=FALSE, fig.width=4.25,fig.height=2.0}
# modifed from http://rpsychologist.com/creating-a-typical-textbook-illustration-of-statistical-power-using-either-ggplot-or-base-graphics

library(ggplot2)
library(grid) # need for arrow()
m1 <- 0  # mu H0
sd1 <- 1 # sigma H0
m2 <- .5 # mu HA
sd2 <- 1 # sigma HA

z_crit <- qnorm(1-(0.05/2), m1, sd1)

# set length of tails
min1 <- m1-sd1*4
max1 <- m1+sd1*4
min2 <- m2-sd2*4
max2 <- m2+sd2*4          
# create x sequence
x <- seq(min(min1,min2), max(max1, max2), .01)
# generate normal dist #1
y1 <- dnorm(x, m1, sd1/12^.5)
# put in data frame
df1 <- data.frame("x" = x, "y" = y1)
# generate normal dist #2
y2 <- dnorm(x, m2, sd2/12^.5)
# put in data frame
df2 <- data.frame("x" = x, "y" = y2)

# combine polygons. 
df1$id <- 2 
df2$id <- 1 
poly <- rbind(df1, df2)
poly$id <- factor(poly$id,  labels=c("h0","h1"))

ggplot(poly, aes(x,y,  group=id)) +
  geom_polygon(show_guide=F, alpha=I(8/10)) +
  geom_line(data=df1, aes(x,y, color="H0", group=NULL, fill=NULL), size=1.5, show_guide=F) + 
  geom_line(data=df2, aes(color="HA", group=NULL, fill=NULL),size=1.5, show_guide=F) +
  scale_color_manual("Group", 
                     values= c("HA" = "#981e0b","H0" = "black")) +
  ggtitle("N = 24") +
  theme(panel.grid.minor = element_blank(),
             panel.grid.major = element_blank(),
             panel.background = element_blank(),
             plot.background = element_rect(fill="#ffffff"),
             panel.border = element_blank(),
             axis.line = element_blank(),
             axis.text.x = element_blank(),
             axis.text.y = element_blank(),
             axis.ticks = element_blank(),
             axis.title.x = element_blank(),
             axis.title.y = element_blank(),
             plot.title = element_text(size=22))
```


#### Significance Test when N = 48
```{r, echo=FALSE, fig.width=4.25,fig.height=2.0}
# modified from http://rpsychologist.com/creating-a-typical-textbook-illustration-of-statistical-power-using-either-ggplot-or-base-graphics

library(ggplot2)
library(grid) # need for arrow()
m1 <- 0  # mu H0
sd1 <- 1 # sigma H0
m2 <- .5 # mu HA
sd2 <- 1 # sigma HA

z_crit <- qnorm(1-(0.05/2), m1, sd1)

# set length of tails
min1 <- m1-sd1*4
max1 <- m1+sd1*4
min2 <- m2-sd2*4
max2 <- m2+sd2*4          
# create x sequence
x <- seq(min(min1,min2), max(max1, max2), .01)
# generate normal dist #1
y1 <- dnorm(x, m1, sd1/24^.5)
# put in data frame
df1 <- data.frame("x" = x, "y" = y1)
# generate normal dist #2
y2 <- dnorm(x, m2, sd2/24^.5)
# put in data frame
df2 <- data.frame("x" = x, "y" = y2)

# combine polygons. 
df1$id <- 2 
df2$id <- 1 
poly <- rbind(df1, df2)
poly$id <- factor(poly$id,  labels=c("h0","h1"))

ggplot(poly, aes(x,y,  group=id)) +
  geom_polygon(show_guide=F, alpha=I(8/10)) +
  geom_line(data=df1, aes(x,y, color="H0", group=NULL, fill=NULL), size=1.5, show_guide=F) + 
  geom_line(data=df2, aes(color="HA", group=NULL, fill=NULL),size=1.5, show_guide=F) +
  scale_color_manual("Group", 
                     values= c("HA" = "#981e0b","H0" = "black")) +
  ggtitle("N = 48") +
  theme(panel.grid.minor = element_blank(),
             panel.grid.major = element_blank(),
             panel.background = element_blank(),
             plot.background = element_rect(fill="#ffffff"),
             panel.border = element_blank(),
             axis.line = element_blank(),
             axis.text.x = element_blank(),
             axis.text.y = element_blank(),
             axis.ticks = element_blank(),
             axis.title.x = element_blank(),
             axis.title.y = element_blank(),
             plot.title = element_text(size=22))
```

You will notice the area of $1-\beta$ increases because as we add sample, the curve gets thinner (SEM), but the distance between them does not change! This effect size is theoretically independent of significance testing as its simply based on the mean difference / standard deviation. If you know the true standard deviation ($\sigma$) than this is a true statement. However, we never know the true standard deviation so we approximate it based on sample ($S$). So our observed estimate of effect size from experimental data is another guess based on the fickleness of probably (which is why we hope meta-analysis which averages over lots of studies is a better estimate).

## Estimating A priori Power
To ensure a specific likelihood, you will find a significant effect ($1-\beta$) we needed to set that value. There are two values that are often chosen .8 and .95. 80% power is the value suggested by Cohen because it's economical. 95% is called high power, and it has been favored of late (but its very expensive and you will see why). Also, you need 3 pieces of information. 1) Effect size (in Cohen's $d$), 2) You $\alpha$ level, and  3) a number of tails you want to set (usually 2-tailed).  

Back to our rhythmic discriminability Lets assume you create a training program to increase students rhythmic discriminability and assume the $d$ = .4 (small-to-medium effect). We can set our $\alpha = .05$ and number of tails (2). and let Cohen's (1988) formulas estimate the same needed. 

#### Power .80
`pwr.t.test` function in the pwr package is built into R already (no need to install it). It is limited to what it can do, but it can do the basic power analysis. For more complex power calculations you will need to download **gpower** a free stand-alone power software or you have to find other packages in R or build a Monte-Carlo simulation (which I might have for you later in the semester ).

`pwr.t.test` works by solving what you leave as `NULL`. 

```{r}
library(pwr)
At.Power.80<-pwr.t.test(n = NULL, d = .4, power = .80, sig.level = 0.05,
               type = c("one.sample"), alternative = c("two.sided"))
At.Power.80
```

We round the value up `r ceiling(At.Power.80$n)`


#### Power .95
```{r}
At.Power.95<-pwr.t.test(n = NULL, d = .4, power = .95, sig.level = 0.05,
               type = c("one.sample"), alternative = c("two.sided"))
At.Power.95
```

We round the value up `r ceiling(At.Power.95$n)`


## Sample size and Power 
Sample size and power at a given effect size have a non-linear relationship

### Power curve d = .4
```{r, echo=FALSE}
library(pwr)
OneSample.T.N<-function(Nrange,D)
  {N<-pwr.t.test(n = Nrange, d = D, power = NULL, sig.level = 0.05,
               type = c("one.sample"), alternative = c("two.sided"))$power
  return(N)
  }

Nrange=seq(2,400,.5)

SStoP<-mapply(OneSample.T.N,Nrange,D=.4)

plot(Nrange,SStoP,'l',lwd=4,
       main="Power Curve, d = .4, a = .05/2", 
  xlab="Sample Size", ylab="Apriori Power", ylim=c(0,1))
abline(h=.80,col="blue",lwd=1)
abline(h=.95,col="red",lwd=1)
```

### Power curve d = .2
```{r, echo=FALSE}
Nrange=seq(2,400,.5)

SStoP<-mapply(OneSample.T.N,Nrange,D=.2)

plot(Nrange,SStoP,'l',lwd=4,
       main="Power Curve, d = .2, a = .05/2", 
  xlab="Sample Size", ylab="Apriori Power", ylim=c(0,1))
abline(h=.80,col="blue",lwd=1)
abline(h=.95,col="red",lwd=1)
```

### Power curve d = .8
```{r, echo=FALSE}
Nrange=seq(2,400,.5)

SStoP<-mapply(OneSample.T.N,Nrange,D=.8)

plot(Nrange,SStoP,'l',lwd=4,
       main="Power Curve, d = .8, a = .05/2", 
  xlab="Sample Size", ylab="Apriori Power", ylim=c(0,1))
abline(h=.80,col="blue",lwd=1)
abline(h=.95,col="red",lwd=1)
```

As you can see, the large the effect the fewer subjects you need.  

### Solve A Priori Power

You can solve A Priori power if you know your N, **True** d, $\alpha$ level
N = 13
d = .4
a = .05, two-tailed

```{r}
library(pwr)
AP.Power.N13<-pwr.t.test(n = 13, d = .4, power = NULL, sig.level = 0.05,
               type = c("one.sample"), alternative = c("two.sided"))
AP.Power.N13
```

A Priori power = `r round(AP.Power.N13$power,3)`


## Power and Effect size
Effect size and power at a given sample size also has a non-linear relationship

### Power Curve n = 25
```{r, echo=FALSE}
library(pwr)
OneSample.T.D<-function(Drange,N)
  {power<-pwr.t.test(n = N, d = Drange, power = NULL, sig.level = 0.05,
               type = c("one.sample"), alternative = c("two.sided"))$power
  return(power)
  }

Drange=seq(.1,1.2,.05)

DtoP<-mapply(OneSample.T.D,Drange,N=25)

plot(Drange,DtoP,'l',lwd=4,
       main="Power Curve, N = 25, a = .05/2", 
  xlab="Effect Size (d)", ylab="Apriori Power", ylim=c(0,1))
abline(h=.80,col="blue",lwd=1)
abline(h=.95,col="red",lwd=1)
```

### Power Curve n = 50
```{r, echo=FALSE}
DtoP<-mapply(OneSample.T.D,Drange,N=50)

plot(Drange,DtoP,'l',lwd=4,
       main="Power Curve, N = 50, a = .05/2", 
  xlab="Effect Size (d)", ylab="Apriori Power", ylim=c(0,1))
abline(h=.80,col="blue",lwd=1)
abline(h=.95,col="red",lwd=1)
```


### Power Curve n = 100
```{r, echo=FALSE}
DtoP<-mapply(OneSample.T.D,Drange,N=100)

plot(Drange,DtoP,'l',lwd=4,
       main="Power Curve, N = 100, a = .05/2", 
  xlab="Effect Size (d)", ylab="Apriori Power", ylim=c(0,1))
abline(h=.80,col="blue",lwd=1)
abline(h=.95,col="red",lwd=1)
```

### Solve for Max Effect Size Detectable

You can solve the largest effect you can detect with your N at a specific a priori power if you know your N, Power level, $\alpha$ level
N = 13
power = .80
a = .05, two-tailed

```{r}
library(pwr)
Eff.Power.N13<-pwr.t.test(n = 13, d = NULL, power = .80, sig.level = 0.05,
               type = c("one.sample"), alternative = c("two.sided"))
Eff.Power.N13
```

Max Effect Size Detectable at .8 power with N = 13 is  `r round(Eff.Power.N13$d,3)`. A very high value (which is bad)



## Power in independent t-tests
The function in R to similar to the functions we have been using, but now it tells you N per group. Also it assumes HOV

Let's go back to our Mozart effect: compare IQ (spatial reasoning) of people right after they listen to Mozart's Sonata for Two Pianos in D Major K. 448 to a control group who listened to [Bach's Brandenburg Concerto No. 6 in B flat major, BWV 1051 Concerto](https://youtu.be/CexJQ8VWJfY?t=15s).  If the effect is for Mozart only, we should not see it work when listening to Bach (matched on modality & tempo, and complexity of dualing voices) 

Simulation: $\mu_{Mozart} = 109$, $\sigma_{Mozart} = 15$, $\mu_{Bach} = 110$, $\sigma_{Bach} = 15$, we extract a sample of $N = 30$. Our t-test (not correcting for HOV)

```{r, echo = FALSE}
set.seed(666)
# extact a sample of 30 people, that has +9 effect on IQ
n = 30
MozartIQ.effect = 109
SDIQ = 15
Mozart.Sample.2 <- rnorm(n,MozartIQ.effect,SDIQ)

BachIQ.effect = 110
SDIQ = 15
Bach.Sample <- rnorm(n,BachIQ.effect,SDIQ)
``` 
 
```{r}
# t-test
MvsB.t.test<-t.test(x= Mozart.Sample.2, y= Bach.Sample,
       alternative = c("two.sided"),
       paired = FALSE, var.equal = TRUE,
       conf.level = 0.95)
MvsB.t.test #call the results

```
 
# Effect Size in Independent Design

We have two sources of variance group 1 and group 2. Cohen originally suggested we can simply use $d = \frac{ M_{exp} - M_{control} }{S_{control}}$, because $S_{control} = S_{exp}$, however later it was suggested to instead use a corrected $d$

$$d = \frac{M_{exp} - M_{control}}{S_{pooled}}$$ 

Why do you think the corrected formula is better? 

## Effect size package
the `effsize package` is useful as it can calculate $d$ in the same way we might apply a t-test in R.

```{r}
library(effsize)

Obs.d<-cohen.d(Mozart.Sample.2, Bach.Sample, pooled=TRUE,paired=FALSE,
                   na.rm=FALSE, hedges.correction=FALSE,
                   conf.level=0.95)
Obs.d
```

Note it can apply hedges correction (g)

$$g = .632\frac{M_{exp} - M_{control}}{S_{pooled}}$$
Note that is am approximation to correct for small samples (note the real formula is more complex, this is just simplified version)

```{r}
HedgeG<-cohen.d(Mozart.Sample.2, Bach.Sample, pooled=TRUE,paired=FALSE,
                   na.rm=FALSE, hedges.correction=TRUE,
                   conf.level=0.95)
HedgeG
```


#### Estiamte sample size at Power .80
Lets say our d = `r round(abs(HedgeG$estimate),3)`

```{r}
ID.At.Power.80<-pwr.t.test(n = NULL, d = abs(HedgeG$estimate), power = .80, sig.level = 0.05,
               type = c("two.sample"), alternative = c("two.sided"))
ID.At.Power.80
```

We round the value up `r ceiling(ID.At.Power.80$n)` and that is the number we need PER GROUP assuming this effect size estimated from our failed experiment. 


#### Estimate sample size at Power .95
```{r}
ID.At.Power.95<-pwr.t.test(n = NULL, d = abs(HedgeG$estimate), power = .95, sig.level = 0.05,
               type = c("two.sample"), alternative = c("two.sided"))
ID.At.Power.95
```

We round the value up `r ceiling(ID.At.Power.95$n)` and that is the number we need PER GROUP assuming this effect size estimated from our failed experiment. 

## Problem with Observed d
Running a single pilot with a small sample to estimate the effect size was a standard practice. However, it can result in problems. We got a small effect size by chance. What if we re-run our study (N = 30) 5000 times? 

In theory my true d = $110 - 109 / 15$ = *.067*. Basically a zero effect. When people look at $d$ they tend to take the absolute value (we will do the same)

```{r, echo=FALSE}
set.seed(666)
# extact a sample of 30 people, that has +9 effect on IQ
n = 30


SimD<-function(n){
  MozartIQ.effect = 109; SDIQ = 15;
  BachIQ.effect = 110; SDIQ = 15;
  Mozart.Sample.2 <- rnorm(n,MozartIQ.effect,SDIQ)
  Bach.Sample <- rnorm(n,BachIQ.effect,SDIQ)
  est.d<-cohen.d(Mozart.Sample.2, Bach.Sample, pooled=TRUE,paired=FALSE,
                   na.rm=FALSE, hedges.correction=FALSE,
                   conf.level=0.95)$estimate
  est.d<-as.vector(est.d)
}

D.distro<-mapply(SimD,rep(30,5000))

hist(abs(D.distro),
  main="Histogram", 
  xlab="Observed cohen's d: \n5000 replications; N=30, true d = .067", ylab="Frequency",
  xlim=c(0,1.2))
abline(v=.067,col="red",lwd=4)
``` 

So in fact our estimated d from that one experiment (observed d = `r round(abs(as.vector(Obs.d$estimate)),3)` had a `r round(mean(ifelse(abs(D.distro) > abs(as.vector(Obs.d$estimate)),1,0)),4)*100`% chance of occurring! Worse yet, there was a `r round(mean(ifelse(abs(D.distro) > .2,1,0)),4)*100`% chance of getting a value above a small effect $d = .2$


# References
Cohen, J. (1962). The statistical power of abnormal-social psychological research: A review. The
*Journal of Abnormal and Social Psychology*, 65, 145-153.


Cohen, J. (1988). *Statistical power analysis for the behavioral sciences (2nd edition)*. Hillsdale,
NJ: Lawrence Erlbaum Associates.

Cohen, J. (1992). A power primer. *Psychological Bulletin*, 112, 155-159.

Hoenig, J. M., & Heisey, D. M. (2001). The abuse of power: the pervasive fallacy of power
calculations for data analysis. *The American Statistician*, 55, 19-24. 

Kicinski, M., Springate, D. A., & Kontopantelis, E. (2015). Publication bias in meta-analyses from the Cochrane Database of Systematic Reviews. Statistics in medicine, 34(20), 2781-2793.

Note: Power graphs adapted from: 
http://rpsychologist.com/creating-a-typical-textbook-illustration-of-statistical-power-using-either-ggplot-or-base-graphics
